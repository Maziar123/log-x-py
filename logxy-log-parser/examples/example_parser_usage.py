#!/usr/bin/env python3
"""
Example: Using logxy-log-parser with logxpy logs.

This script demonstrates how to use the logxy-log-parser library
to parse, analyze, and visualize logxpy log files.
"""

from pathlib import Path
from logxy_log_parser import (
    LogParser,
    LogFilter,
    LogAnalyzer,
    parse_log,
    check_log,
)


def main() -> None:
    """Run parser examples."""
    # Path to example log file (generated by logxpy)
    log_file = Path("../examples-log-view/complete-example-01/example_02_actions.log")

    if not log_file.exists():
        print(f"Log file not found: {log_file}")
        print("Run example_02_actions.py first to generate the log.")
        return

    print("=" * 60)
    print("LogXPy Log Parser Examples")
    print("=" * 60)

    # 1. Quick parsing with one-line function
    print("\n1. Quick Parse (one-line)")
    print("-" * 40)
    entries = parse_log(log_file)
    print(f"   Parsed {len(entries)} entries")

    # 2. Detailed parsing with error handling
    print("\n2. Detailed Parse with Validation")
    print("-" * 40)
    result = check_log(log_file)
    print(f"   Valid: {result.is_valid}")
    print(f"   Parsed count: {result.parsed_count}")
    print(f"   Total lines: {result.total_lines}")
    print(f"   Skipped count: {result.skipped_count}")
    print(f"   Validation errors: {len(result.validation_errors)}")
    if result.validation_errors:
        for error in result.validation_errors[:3]:
            print(f"      - {error}")

    # 3. Using LogParser for more control
    print("\n3. Using LogParser")
    print("-" * 40)
    parser = LogParser(log_file)
    entries = parser.parse()

    # Show first few entries
    for i, entry in enumerate(entries[:3]):
        print(f"   Entry {i + 1}:")
        print(f"      - Level: {entry.level.value}")
        print(f"      - Task UUID: {entry.task_uuid[:8]}...")
        print(f"      - Task Level: {entry.task_level}")
        print(f"      - Action: {entry.action_type or 'N/A'}")
        print(f"      - Message: {entry.message or entry.message_type or 'N/A'}")

    # 4. Filtering logs
    print("\n4. Filtering Logs")
    print("-" * 40)

    # Filter by level
    info_entries = LogFilter(entries).by_level("info")
    print(f"   Info entries: {len(info_entries)}")

    # Filter by action type
    payment_actions = LogFilter(entries).by_action_type("payment*")
    print(f"   Payment actions: {len(payment_actions)}")

    # Filter failed actions
    failed = LogFilter(entries).failed_actions()
    print(f"   Failed actions: {len(failed)}")

    # Filter slow actions
    slow = LogFilter(entries).slow_actions(threshold=0.05)
    print(f"   Slow actions (>0.05s): {len(slow)}")

    # Combine filters - get failed payment actions
    payment_failed = LogFilter(payment_actions).failed_actions()
    print(f"   Failed payment actions: {len(payment_failed)}")

    # 5. Analyzing logs
    print("\n5. Log Analysis")
    print("-" * 40)
    analyzer = LogAnalyzer(entries)

    # Get error summary
    error_summary = analyzer.error_summary()
    print(f"   Total errors: {error_summary.total_count}")
    print(f"   Unique error types: {error_summary.unique_types}")

    # Level distribution (manual count)
    from collections import Counter
    level_counts = Counter(e.level.value for e in entries)
    print(f"\n   Level distribution:")
    for level, count in sorted(level_counts.items()):
        print(f"      - {level.upper()}: {count}")

    # Performance analysis
    print(f"\n   Performance:")
    slowest = analyzer.slowest_actions(n=3)
    for stat in slowest:
        print(f"      - {stat.action_type}: {stat.duration:.3f}s")

    # Deepest nesting
    print(f"\n   Structure:")
    print(f"      - Deepest nesting: {analyzer.deepest_nesting()}")
    widest = analyzer.widest_tasks()[:3]
    for task_uuid, width in widest:
        print(f"      - Task {task_uuid[:8]}...: {width} entries")

    # 6. Building task tree
    print("\n6. Task Tree Visualization")
    print("-" * 40)
    from logxy_log_parser import TaskTree

    task_uuids = list(set(e.task_uuid for e in entries))
    if task_uuids:
        tree = TaskTree.from_entries(entries, task_uuids[0])
        print("   Tree (first task):")
        for line in tree.visualize("text").split("\n")[:10]:
            print(f"      {line}")

        tree_stats = tree.get_stats()
        print(f"\n   Tree stats:")
        print(f"      - Nodes: {tree_stats['total_nodes']}")
        print(f"      - Messages: {tree_stats['total_messages']}")
        print(f"      - Max depth: {tree_stats['max_depth']}")
        print(f"      - Completed: {tree_stats['completed']}")
        print(f"      - Failed: {tree_stats['failed']}")

    # 7. Export options
    print("\n7. Export Options")
    print("-" * 40)

    # Export using LogEntries.to_dict()
    import json

    # Export to JSON
    with open("/tmp/parser_output.json", "w") as f:
        json.dump([e.to_dict() for e in entries], f, indent=2, default=str)
    print(f"   Exported to JSON: /tmp/parser_output.json")

    # Export to CSV using the export_csv from CLI module
    from logxy_log_parser.cli import export_csv
    export_csv(entries, "/tmp/parser_output.csv")
    print(f"   Exported to CSV: /tmp/parser_output.csv")

    # You can also use the CLI directly:
    print(f"\n   CLI usage examples:")
    print(f"      logxy-query query {log_file.name} --level error")
    print(f"      logxy-query tree {log_file.name}")
    print(f"      logxy-query analyze {log_file.name}")

    print("\n" + "=" * 60)
    print("Examples complete!")
    print("=" * 60)


if __name__ == "__main__":
    main()
